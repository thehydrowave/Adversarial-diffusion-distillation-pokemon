# -*- coding: utf-8 -*-
"""ADD_part2_Naruto_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mvrTMU15V408FVQMwiOBnnfX7VoekZ1P

<a href="https://colab.research.google.com/github/thehydrowave/Adversarial-diffusion-distillation-pokemon/blob/main/ADD_part2_Naruto_Project.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# üß† Adversarial Diffusion Distillation (ADD) - Projet Naruto

Ce notebook impl√©mente l'entra√Ænement d'un mod√®le student rapide √† partir d'un mod√®le Stable Diffusion LoRA (teacher), selon le papier [Adversarial Diffusion Distillation (ADD)](https://arxiv.org/abs/2311.17042).
"""

!pip install numpy==1.26.4 diffusers==0.25.1 transformers==4.37.2 accelerate==0.27.2 \
datasets==2.17.0 huggingface_hub==0.25.0 peft==0.7.1 fsspec==2023.6.0 \
torch==2.2.2 torchvision==0.17.2 pandas==2.2.2 timm --no-cache-dir --force-reinstall

import numpy as np
from diffusers import StableDiffusionPipeline
import torch
import torch.nn as nn
import timm
from datasets import load_dataset
from torchvision import transforms
from PIL import Image
from torch.utils.data import DataLoader
from tqdm import tqdm
import copy

print(np.__version__)
print("‚úÖ Tout fonctionne avec diffusers.")

# üîÅ Injection LoRA et chargement manuel depuis .bin
from diffusers.models.attention_processor import LoRAAttnProcessor

# Initialize teacher before using it
teacher = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda" if torch.cuda.is_available() else "cpu")

def inject_lora_modules(unet):
    print("‚öôÔ∏è Injection des modules LoRA...")
    cross_attention_dim = unet.config.cross_attention_dim
    attention_dims = {
        "mid_block": unet.config.block_out_channels[-1],
        "up_blocks.3": unet.config.block_out_channels[0],
        "up_blocks.2": unet.config.block_out_channels[1],
        "up_blocks.1": unet.config.block_out_channels[2],
        "up_blocks.0": unet.config.block_out_channels[3],
        "down_blocks.0": unet.config.block_out_channels[0],
        "down_blocks.1": unet.config.block_out_channels[1],
        "down_blocks.2": unet.config.block_out_channels[2],
        "down_blocks.3": unet.config.block_out_channels[3],
    }

    attn_processors = {}
    for name, processor in unet.attn_processors.items():
        for block_key in attention_dims:
            if name.startswith(block_key):
                hidden_size = attention_dims[block_key]
                is_cross_attention = name.endswith("attn2.processor")

                attn_processors[name] = LoRAAttnProcessor(
                    hidden_size=hidden_size,
                    cross_attention_dim=cross_attention_dim if is_cross_attention else None
                )
                break
        else:
            attn_processors[name] = processor

    unet.set_attn_processor(attn_processors)
    print(f"‚úÖ LoRA inject√© dans {sum(isinstance(p, LoRAAttnProcessor) for p in attn_processors.values())} couches.")

def load_lora_weights_from_bin(unet, lora_path):
    print(f"üîç Chargement des poids LoRA depuis : {lora_path}")
    lora_weights = torch.load(lora_path)
    updated = 0
    for full_key, module in unet.attn_processors.items():
        if hasattr(module, 'load_state_dict'):
            prefix = full_key + "."
            sub_state_dict = {k[len(prefix):]: v for k, v in lora_weights.items() if k.startswith(prefix)}
            if sub_state_dict:
                try:
                    module.load_state_dict(sub_state_dict, strict=False)
                    updated += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur pour {full_key} : {e}")
    print(f"‚úÖ {updated}/{len(unet.attn_processors)} modules LoRA mis √† jour.")

inject_lora_modules(teacher.unet)
load_lora_weights_from_bin(teacher.unet, "./pytorch_lora_weights.bin")

# üß† Initialiser le student avec m√™mes modules LoRA
student = copy.deepcopy(teacher)
student.scheduler.set_timesteps(num_inference_steps=4)
# student.train()  # ‚ùå Pas n√©cessaire, les modules entra√Ænables sont g√©r√©s via student.unet.train()

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        # Use a model that accepts 64x64 images
        # or modify the existing one to accept 64x64
        self.backbone = timm.create_model('resnet18', pretrained=True, num_classes=1) # Example with ResNet18
        #self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=1, img_size=64)  # Tiny ViT

    def forward(self, x):
        return self.backbone(x)

# Define the device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
discriminator = Discriminator().to(device)

# üì• Dataset Naruto
dataset = load_dataset("lambdalabs/naruto-blip-captions", split="train")
dataset = dataset.select(range(128))

transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])

def transform_example(example):
    image = example["image"].convert("RGB")
    example["pixel_values"] = transform(image)
    return example

dataset = dataset.map(transform_example)

def collate_fn(batch):
    pixel_values = torch.stack([
        torch.tensor(x["pixel_values"]) if not isinstance(x["pixel_values"], torch.Tensor) else x["pixel_values"]
        for x in batch
    ])
    prompts = [x["text"] for x in batch]
    return pixel_values, prompts


train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# üîÅ Training Loop ADD (SDS + GAN + distillation interm√©diaire)
lambda_adv = 0.5
optimizer_student = torch.optim.Adam(student.unet.parameters(), lr=1e-5)
optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=1e-5)
adversarial_loss_fn = nn.BCEWithLogitsLoss()
mse_loss_fn = nn.MSELoss()

# Enable mixed precision training
# Removed GradScaler as it's not needed for FP16 training
# scaler = torch.cuda.amp.GradScaler()  # Create a GradScaler object

# Gradient Accumulation (Optional, uncomment if needed)
# accum_iter = 2  # Accumulate gradients over 2 iterations

for epoch in range(2):
    loop = tqdm(train_dataloader, desc=f"Epoch {epoch}")
    for idx, (pixel_values, prompts) in enumerate(loop): # Added idx for gradient accumulation
        pixel_values = pixel_values.to(device, dtype=torch.float16)
        batch_size = pixel_values.shape[0]

        # Encoder les prompts
        inputs = teacher.tokenizer(prompts, padding="max_length", truncation=True, return_tensors="pt").to(device)
        encoder_hidden_states = teacher.text_encoder(**inputs).last_hidden_state

        # Sample un timestep t al√©atoire
        t = torch.randint(0, teacher.scheduler.config.num_train_timesteps, (batch_size,), device=device).long()

        # Encode les images en latents (teacher)
        with torch.no_grad():
            latents = teacher.vae.encode(pixel_values).latent_dist.sample() * 0.18215
            noise = torch.randn_like(latents)
            x_t = teacher.scheduler.add_noise(latents, noise, t)
            target = noise

        # --- Entra√Ænement du Discriminateur ---
        with torch.no_grad():
            gen_latents = student.vae.decode(latents).sample
            gen_images = torch.clamp((gen_latents / 2 + 0.5), 0, 1)

        discriminator.zero_grad()
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # Cast the inputs to float32 for the discriminator
        real_output = discriminator(pixel_values.float())
        fake_output = discriminator(gen_images.detach().float())
        gen_output = discriminator(gen_images.float())

        d_loss_real = adversarial_loss_fn(real_output, real_labels)
        d_loss_fake = adversarial_loss_fn(fake_output, fake_labels)
        d_loss = (d_loss_real + d_loss_fake) / 2

        d_loss.backward()
        optimizer_disc.step()

        # --- Entra√Ænement du Student ---
        student.unet.train()
        optimizer_student.zero_grad()

        # Use autocast for mixed precision training
        with torch.autocast(device_type='cuda', dtype=torch.float16):
            noise_pred = student.unet(x_t, t, encoder_hidden_states).sample
            loss_sds = mse_loss_fn(noise_pred, target)
            gen_output = discriminator(gen_images.float())
            loss_adv = adversarial_loss_fn(gen_output, real_labels)
            s_loss = loss_sds + lambda_adv * loss_adv

        # Gradient Accumulation (Optional, uncomment if needed)
        # s_loss = s_loss / accum_iter  # Normalize the loss

        # Removed scaler as it's not needed for FP16 training
        # scaler.scale(s_loss).backward()

        s_loss.backward()
        # if (idx + 1) % accum_iter == 0: # Uncomment for gradient accumulation
        # Removed the manual casting of gradients to FP16
        # scaler.step(optimizer_student)
        # scaler.update()

        # optimizer_student.zero_grad() # Uncomment for gradient accumulation
        # Removed scaler as it's not needed for FP16 training
        # scaler.step(optimizer_student)
        # scaler.update()

        optimizer_student.step()


        loop.set_postfix(d_loss=d_loss.item(), s_loss=s_loss.item(), sds=loss_sds.item())

# üîÅ Training Loop ADD (SDS + GAN + distillation interm√©diaire)
lambda_adv = 0.5
optimizer_student = torch.optim.Adam(student.unet.parameters(), lr=1e-5)
optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=1e-5)
adversarial_loss_fn = nn.BCEWithLogitsLoss()
mse_loss_fn = nn.MSELoss()

for epoch in range(2):
    loop = tqdm(train_dataloader, desc=f"Epoch {epoch}")
    for pixel_values, prompts in loop:
        pixel_values = pixel_values.to(device, dtype=torch.float16)
        batch_size = pixel_values.shape[0]

        # Encoder les prompts
        inputs = teacher.tokenizer(prompts, padding="max_length", truncation=True, return_tensors="pt").to(device)
        encoder_hidden_states = teacher.text_encoder(**inputs).last_hidden_state

        # Sample un timestep t al√©atoire
        t = torch.randint(0, teacher.scheduler.config.num_train_timesteps, (batch_size,), device=device).long()

        # Encode les images en latents (teacher)
        with torch.no_grad():
            latents = teacher.vae.encode(pixel_values).latent_dist.sample() * 0.18215
            noise = torch.randn_like(latents)
            x_t = teacher.scheduler.add_noise(latents, noise, t)
            target = noise

        # --- Entra√Ænement du Discriminateur ---
        with torch.no_grad():
            gen_latents = student.vae.decode(latents).sample
            gen_images = torch.clamp((gen_latents / 2 + 0.5), 0, 1)

        discriminator.zero_grad()
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        real_output = discriminator(pixel_values.float())
        fake_output = discriminator(gen_images.detach().float())
        gen_output = discriminator(gen_images.float())

        d_loss_real = adversarial_loss_fn(real_output, real_labels)
        d_loss_fake = adversarial_loss_fn(fake_output, fake_labels)
        d_loss = (d_loss_real + d_loss_fake) / 2

        d_loss.backward()
        optimizer_disc.step()

        # --- Entra√Ænement du Student ---
        student.unet.train()
        optimizer_student.zero_grad()

        # SDS Loss = MSE entre bruit pr√©dits et bruit r√©el
        noise_pred = student.unet(x_t, t, encoder_hidden_states).sample
        loss_sds = mse_loss_fn(noise_pred, target)

        # GAN loss
        gen_output = discriminator(gen_images)
        loss_adv = adversarial_loss_fn(gen_output, real_labels)

        # Total loss
        s_loss = loss_sds + lambda_adv * loss_adv
        s_loss.backward()
        optimizer_student.step()

        loop.set_postfix(d_loss=d_loss.item(), s_loss=s_loss.item(), sds=loss_sds.item())

"""---------------------------------------------------------------------------"""

import numpy as np
print(np.__version__)

from diffusers import StableDiffusionPipeline
print("‚úÖ Tout fonctionne avec diffusers.")

# üì¶ Installation des d√©pendances
!pip install diffusers transformers accelerate datasets timm --quiet

# üîß Imports & chargement du mod√®le teacher
import torch
from diffusers import StableDiffusionPipeline

device = 'cuda' if torch.cuda.is_available() else 'cpu'
teacher = StableDiffusionPipeline.from_pretrained(
    'runwayml/stable-diffusion-v1-5',
    torch_dtype=torch.float16
).to(device)

# Charger les poids LoRA Naruto
teacher.unet.load_attn_procs('./naruto_lora_weights')
teacher.eval()

# üß† Initialiser le student
import copy
student = copy.deepcopy(teacher)
student.scheduler.set_timesteps(num_inference_steps=4)
student.train()

# ü§ñ Discriminateur bas√© sur ViT (via timm)
import timm
import torch.nn as nn

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = timm.create_model('vit_base_patch16_224', pretrained=True)
        self.head = nn.Linear(self.backbone.num_features, 1)

    def forward(self, x):
        features = self.backbone(x)
        return self.head(features)

discriminator = Discriminator().to(device)

"""## üîÅ Boucle d'entra√Ænement ADD simplifi√©e (√† compl√©ter selon dataset Naruto)"""

# üîÑ Training loop (SQUELETTE √Ä COMPL√âTER)
lambda_adv = 0.5
optimizer_student = torch.optim.Adam(student.unet.parameters(), lr=1e-5)
optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=1e-5)

# ‚ûï √Ä compl√©ter : charger le dataset Naruto BLIP captions, g√©n√©rer x_s, calculer pertes
# ‚ûï Ajout de SDS loss + hinge loss sur output du discriminateur
# ‚ûï Entra√Ænement altern√© student/discriminator